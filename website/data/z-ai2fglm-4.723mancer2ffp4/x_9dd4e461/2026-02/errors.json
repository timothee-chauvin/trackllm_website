{"seen_errors":[{"http_code":0,"message":"Timeout after 30.0s"},{"http_code":429,"message":"{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"z-ai/glm-4.7 is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Mancer 2\",\"is_byok\":false,\"retry_after_seconds\":15}}"},{"http_code":503,"message":"{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\\"error\\\":{\\\"data\\\":null,\\\"message\\\":\\\"Model's inferencers are temporarily inaccessible.\\\",\\\"type\\\":\\\"MODEL_OFFLINE\\\"}}\\n\",\"provider_name\":\"Mancer 2\",\"is_byok\":false}}"},{"http_code":200,"message":"No logprobs returned"}]}